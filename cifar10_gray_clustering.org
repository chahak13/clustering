#+TITLE: CIFAR-10 Clustering of Grayscale images
#+AUTHOR: Chahak Mehta
#+property: header-args :session /ssh:pho-sach:/oden/cmehta/.local/share/jupyter/runtime/kernel-5154355c-a8c6-412b-a7ef-28588cbde93b.json :async yes :eval no-export :exports both

We have generated grayscale CIFAR-10 images and stored them in pickle files. As of now, we've done 1 batch of 10000 images.

#+begin_src jupyter-python
import pickle
import numpy as np

batch = pickle.load(open("/workspace/CHAHAK/dsml/project/data/cifar-10-batches-py/gray_data_batch_1.pkl", "rb"), encoding="bytes")
#+end_src

#+RESULTS:

#+begin_src jupyter-python
batch.keys()
#+end_src

#+RESULTS:
: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])

We can now use these grayscale images to perform clustering. To do this, first we need to transform the 32x32 images into a single row vector of high dimension.

#+begin_src jupyter-python
image_data = batch[b'data'].reshape(10000, -1, 1)
image_data = image_data.squeeze()
image_data.shape
#+end_src

#+RESULTS:
| 10000 | 1024 |

#+begin_src jupyter-python
from sklearn.cluster import KMeans
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

kmeans = KMeans(n_clusters=10)
estimator = make_pipeline(StandardScaler(), kmeans).fit(image_data)
kmeans.labels_[:10]
#+end_src

#+RESULTS:
: array([2, 2, 3, 1, 4, 4, 9, 0, 0, 4], dtype=int32)

To check homogenity of the clusters, we use the ~sklearn.metrics.homogeneity_score~. It is also independent of the absolute values of the clusters, so that also helps us since it checks permutations of the cluster labels.

#+begin_src jupyter-python
from sklearn.metrics import homogeneity_score

homogeneity_score(batch[b'labels'], estimator[-1].labels_)
#+end_src

#+RESULTS:
: 0.06144550072712146

** SVD on grayscale images

We now apply SVD to these images to check potential impact on homogeneity score and time/data size.
#+begin_src jupyter-python
from tqdm import tqdm

def compress_img(img, k):
    img = img.reshape(32, 32)
    U, D, V = np.linalg.svd(img, full_matrices=False)
    compressed_image = U[:, :k] @ np.diag(D[:k]) @ V[:k, :]
    return compressed_image

compressed_images = []
k = 10
for img in tqdm(image_data):
    cimg = compress_img(img, 10)
    cimg_vector = cimg.reshape(-1,)
    compressed_images.append(cimg_vector)

compressed_images = np.asarray(compressed_images)
compressed_images.shape
#+end_src

#+RESULTS:
:RESULTS:
: 100%|███████████████████████████████████████████████████| 10000/10000 [00:02<00:00, 3807.38it/s]
| 10000 | 1024 |
:END:

#+begin_src jupyter-python
kmeans = KMeans(n_clusters=10)
svd_estimator = make_pipeline(StandardScaler(), kmeans).fit(image_data)
homogeneity_score(batch[b'labels'], svd_estimator[-1].labels_)
#+end_src

#+RESULTS:
: 0.062220044439389936

#+begin_src jupyter-python
import numpy as np
import matplotlib.pyplot as plt

test = image_data[0].reshape(32, 32)
_, d, _ = np.linalg.svd(test)
fig, ax = plt.subplots()
ax.plot(d, 'ko', markersize=2, label="Singular values")
ax.grid()
ax.legend()
#+end_src

#+RESULTS:
:RESULTS:
: <matplotlib.legend.Legend at 0x7fdf1a782880>
[[file:./.ob-jupyter/0914f91d6f7aa91e3325e30fde25296029b0e31d.png]]
:END:
